{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-08T08:36:32.722739Z",
     "start_time": "2024-05-08T08:36:32.719878Z"
    }
   },
   "source": [
    "import pickle\n",
    "\n",
    "from langchain.document_loaders import TextLoader"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:06:30.807877Z",
     "start_time": "2024-05-06T19:06:30.800635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = TextLoader(\"nvidia_news_2.txt\")\n",
    "data = loader.load()\n",
    "data"
   ],
   "id": "c049cd6dc03c72b0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"The chipmaker has delivered phenomenal gains over the past five years, and investors can expect the rally to continue in the future.\\n\\nNvidia (NVDA 3.46%) stock has crushed the broader market handsomely over the past five years with eye-popping gains of more than 1,800% -- which is significantly higher than the S&P 500 index's gains of 71% -- and a big reason behind the chip giant's terrific surge is its dominant position in the artificial intelligence (AI) chip market.\\n\\nHowever, can Nvidia sustain its AI-driven growth for the next five years and deliver more gains to investors? Let's find out.\\n\\n\\nCollapse\\n\\nNASDAQ: NVDA\\nNvidia\\nToday's Change\\n(3.46%) $29.72\\nCurrent Price\\n$887.89\\nYTD\\n1W\\n1M\\n3M\\n6M\\n1Y\\n5Y\\nPRICE\\nVS S&P\\n NVDA\\nKEY DATA POINTS\\nMarket Cap\\n$2,220B\\nDay's Range\\n$870.40 - $892.81\\n52wk Range\\n$280.46 - $974.00\\nVolume\\n39,834,072\\nAvg Vol\\n53,634,351\\nGross Margin\\n72.72%\\nDividend Yield\\n0.02%\\nThe AI chip market's massive growth should be a long-term tailwind for Nvidia\\nAllied Market Research estimates that the global AI chip market was worth just under $15 billion in 2022. However, by 2032, the market research firm expects sales of AI chips to generate annual revenue of nearly $384 billion. Nvidia is in the driver's seat in this space, controlling an estimated 94% of the AI chip market last year, per Vijay Rakesh of Mizuho Securities.\\n\\nNvidia's first-mover advantage in AI chips has paid off handsomely for the company. Its graphics cards have been extremely popular and deployed by major cloud computing providers and start-ups to train AI models, including ChatGPT. As a result, Nvidia's data center revenue has been growing at a stunning rate. In fiscal 2024, which ended on Jan. 28, 2024, the company reported data center revenue of $47.5 billion, up a massive 217% from the previous year.\\n\\nNvidia, however, is expected to cede some ground in the AI chip market to rivals. Rakesh, for instance, expects the company's AI chip share to slip to 75% over the next couple of years as the likes of Intel (INTC 1.28%) and Advanced Micro Devices (AMD 3.04%) bring competitive offerings into the market. AMD witnessed an 80% year-over-year increase in sales of its data center chips in the first quarter of 2024 to $2.3 billion, thanks to the growing demand for its AI accelerators.\\n\\nThe Nvidia competitor expects to sell $4 billion worth of AI graphics cards this year, which is double its initial estimate of $2 billion. Analysts, however, expect AMD to end the year with as much as $8 billion in revenue from sales of AI-focused data center chips.\\n\\nIntel management also pointed out on the company's latest earnings conference call that its new Gaudi 3 data center accelerators are gaining traction and could generate $500 million in revenue in the second half of 2024, followed by a stronger showing next year. Nvidia, however, is expected to be in a league of its own.\\n\\nAccording to market research firm Omdia, the chipmaker could sell $87 billion worth of data center graphics cards this year. That would be a huge jump from the $34 billion the company generated through data center chip sales last year. Even better, Rakesh forecasts Nvidia could sell $280 billion worth of data center graphics cards in 2027 despite the potential loss in market share. That's not surprising, considering how fast this space is growing.\\n\\nIn simple words, there is enough room for Nvidia to grow in the AI chip market even if the competition gets stronger. Throw in the potential growth the company could witness in other markets, such as gaming, it is easy to see why analysts expect Nvidia to clock healthy earnings growth for the next five years.\\n\\nHow much upside can investors expect over the next five years?\\nConsensus estimates predict Nvidia's earnings will increase at an annual rate of just over 35% for the next five years. Based on the company's fiscal 2024 earnings of $12.96 per share, its bottom line could jump to $58.11 per share after five years, assuming it does increase at the predicted rate.\\n\\nThe good part is that the company seems capable of delivering the growth that analysts are expecting from it based on the points discussed above and Nvidia's commanding pricing power in the AI chip space. Not surprisingly, analysts bumped up Nvidia's earnings estimates significantly for the current and the next two fiscal years, as seen in the chart below.\\n\\nNVDA EPS Estimates for Current Fiscal Year Chart\\n\\nNVDA EPS ESTIMATES FOR CURRENT FISCAL YEAR DATA BY YCHARTS\\n\\nAs such, there is a good chance that Nvidia's earnings could indeed touch the projected $58.11 per share after five years. Multiplying the projected earnings with Nvidia's five-year average forward earnings multiple of 39 suggests that its stock price could hit $2,266 per share (barring any stock splits or other events) after five years. That would translate into a jump of 162% from current levels.\\n\\nSo, investors who haven't bought this AI stock yet can still consider doing so, as its impressive rally seems sustainable over the next five years.\\n\\nShould you invest $1,000 in Nvidia right now?\\nBefore you buy stock in Nvidia, consider this:\\n\\nThe Motley Fool Stock Advisor analyst team just identified what they believe are the 10 best stocks for investors to buy now… and Nvidia wasn’t one of them. The 10 stocks that made the cut could produce monster returns in the coming years.\\n\\nConsider when Nvidia made this list on April 15, 2005... if you invested $1,000 at the time of our recommendation, you’d have $544,015!*\\n\\nAs you can see, Stock Advisor members who bought and held our recommendation have been handsomely rewarded. Don’t miss out on the latest 10 top list.\", metadata={'source': 'nvidia_news_2.txt'})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:06:30.813483Z",
     "start_time": "2024-05-06T19:06:30.809573Z"
    }
   },
   "cell_type": "code",
   "source": "data[0].metadata",
   "id": "cf097c22438eaa84",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'nvidia_news_2.txt'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:06:30.821215Z",
     "start_time": "2024-05-06T19:06:30.814643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "csv_loader = CSVLoader(\"movies.csv\")\n",
    "csv_data = csv_loader.load()\n",
    "len(csv_data)"
   ],
   "id": "f482ee0ba1d3c28e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:06:30.828062Z",
     "start_time": "2024-05-06T19:06:30.824040Z"
    }
   },
   "cell_type": "code",
   "source": "csv_data[0]",
   "id": "5fae7e9da7fc2a8d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='movie_id: 101\\ntitle: K.G.F: Chapter 2\\nindustry: Bollywood\\nrelease_year: 2022\\nimdb_rating: 8.4\\nstudio: Hombale Films\\nlanguage_id: 3\\nbudget: 1\\nrevenue: 12.5\\nunit: Billions\\ncurrency: INR', metadata={'source': 'movies.csv', 'row': 0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:06:30.833141Z",
     "start_time": "2024-05-06T19:06:30.829533Z"
    }
   },
   "cell_type": "code",
   "source": "from langchain.document_loaders import UnstructuredURLLoader",
   "id": "b4a5a7c7e77674df",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:06:37.371071Z",
     "start_time": "2024-05-06T19:06:30.834422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url_loader = UnstructuredURLLoader(urls=[\n",
    "    \"https://www.fool.com/investing/2024/05/04/where-will-nvidia-stock-be-in-5-years/\",\n",
    "    \"https://jobs.danfoss.com/job/Copenhagen-AI-Engineer/793304102/\",\n",
    "])\n",
    "web_data = url_loader.load()"
   ],
   "id": "67898ab78b5e8991",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/felixachter/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/felixachter/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:07:18.175278Z",
     "start_time": "2024-05-06T19:07:18.171658Z"
    }
   },
   "cell_type": "code",
   "source": "web_data[1]",
   "id": "d11491f1e2cd1b82",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='We use cookies to offer you the best possible website experience. Your cookie preferences will be stored in your browser’s local storage. This includes cookies necessary for the website\\'s operation. Additionally, you can freely decide and change any time whether you accept cookies or choose to opt out of cookies to improve the website\\'s performance, as well as cookies used to display content tailored to your interests. Your experience of the site and the services we are able to offer may be impacted if you do not accept all cookies.\\n\\nPress Tab to Move to Skip to Content Link\\n\\nSkip to main content\\n\\nSearch Jobs\\n\\nJoin Danfoss Talent Community\\n\\nCareers at Danfoss\\n\\nJob Locations\\n\\nLanguage\\n\\nDansk (Danmark)\\n\\nDeutsch (Deutschland)\\n\\nEnglish (United Kingdom)\\n\\nEspañol (España)\\n\\nFrançais (France)\\n\\nPolski\\n\\nРусский язык (Россия)\\n\\nSlovencina (Slovensko)\\n\\n简体中文 (中国大陆)\\n\\nView profile\\n\\nSearch Jobs\\n\\nJoin Danfoss Talent Community\\n\\nCareers at Danfoss\\n\\nJob Locations\\n\\nLanguage\\n\\nDansk (Danmark)\\n\\nDeutsch (Deutschland)\\n\\nEnglish (United Kingdom)\\n\\nEspañol (España)\\n\\nFrançais (France)\\n\\nPolski\\n\\nРусский язык (Россия)\\n\\nSlovencina (Slovensko)\\n\\n简体中文 (中国大陆)\\n\\nView profile\\n\\nLanguage\\n\\nDansk (Danmark)\\n\\nDeutsch (Deutschland)\\n\\nEnglish (United Kingdom)\\n\\nEspañol (España)\\n\\nFrançais (France)\\n\\nPolski\\n\\nРусский язык (Россия)\\n\\nSlovencina (Slovensko)\\n\\n简体中文 (中国大陆)\\n\\nView profile\\n\\nSearch Jobs\\n\\nJoin Danfoss Talent Community\\n\\nJoin Danfoss Student Community\\n\\nCareers at Danfoss\\n\\nJob Locations\\n\\nSearch by Keyword\\n\\nSearch by Location\\n\\nSearch by Postal Code\\n\\nSearch by Location\\n\\nSearch by Postal Code\\n\\nDistance\\n\\nSearch by Postal Code\\n\\nSearch by Location\\n\\nShow More Options\\n\\nLoading...\\n\\nSelect how often (in days) to receive an alert:\\n\\nCreate Alert\\n\\nSelect how often (in days) to receive an alert:\\n\\nApply now »\\n\\nAI Engineer\\n\\nRequisition ID:\\n\\n38420\\n\\nJob Location(s):\\n\\nCopenhagen, DK\\n\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tKolding, DK\\n\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tNordborg, DK\\n\\t\\t\\t\\t\\t\\n\\n\\t\\t\\t\\t\\t\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tLjubljana, SI\\n\\nJob Description\\rAre you passionate about advancing our digital solutions that impact the daily work of installers and design engineers across the world? And do you have a proactive mindset together with a strong understanding of AI models and systems? Then you might be our new AI Engineer in Danfoss Climate Solutions. \\n\\n\\xa0\\n\\nYou will become part of an international company and join the Digital Tools team. The team has a proven track record of developing and maintaining two significant mobile applications, Ref Tools and Installer App. These applications, tailored for cooling and heating installers, have garnered more than 350,000 active HVAC installers globally each month (over 1 million unique users yearly), demonstrating their widespread success and effectiveness. Additionally, our team is involved in creating various product selection tools, ranging from straightforward selectors to sophisticated, application-specific solutions.\\n\\n\\xa0\\n\\nWe have recently started working on various AI and ML projects, including internal LLMs, computer vision applications, and sales forecasting initiatives. These projects aim to address key challenges within our operations, and you will play a key role in ensuring their success. \\nJob Responsibilities\\rYour tasks will vary from project to project, and may include tasks such as:\\n\\n\\n\\tWork on topics such as LLM output verification, guard rails, vector embedding and knowledge graph-based data retrieval systems, agent orchestration, data connectors to different systems and databases and LLM inference.\\n\\tDevelop and optimize AI algorithms and models for processing structured and unstructured data.\\n\\tCollaborate with cross-functional teams to understand business requirements and translate them into technical solutions.\\n\\tDesign and implement scalable AI systems.\\n\\tEnsure the quality and reliability of AI solutions through rigorous testing and validation.\\n\\tDocument AI models, algorithms, and systems.\\n\\tImplement and continuously improve release processes, internal tooling, and test automation.\\n\\tWork with data scientists, developers, product owners and other stakeholders from the business to succeed in our common projects.\\n\\n\\n\\xa0\\n\\nAt Danfoss, we believe that a diverse and inclusive workplace fosters creativity, innovation, and a broader perspective in decision-making. When you consider this job posting, do you feel like your profile is not a perfect match? Numerous studies have found that women and people of color are more likely to apply only when they meet all requirements listed in the job posting. Even if you do not check all the boxes, we encourage you to apply anyway. We are curious to find out how you can bring new insights to the role or to Danfoss as an organization.\\nBackground & Skills\\rTo be considered for this position, you need to have a Bachelor’s or Master’s in Data Science, Computer Science, Engineering, or a related field, and you have a deep understanding of the AI domain, including up-to-date knowledge of current technologies, their capabilities and limitations, and their application in practical scenarios. Furthermore, you have:\\n\\n\\xa0\\n\\n\\n\\tHands-on experience with developing and deploying AI solutions.\\n\\tExperience with MLOps as a concept, and practical implementation. \\n\\tKnowledge of data storage technologies. \\n\\tKnowledge of architectural design and principles.\\n\\tOverview of tools and service offerings from different vendors, e.g., Azure, Google Cloud, Databricks. \\n\\n\\nAs a person, you are openminded, curious, and you have a willingness to learn and develop.\\n\\nYou can deal with complexity and find compromises between multiple and sometimes conflicting interests in a global environment across cultures. You have strong stakeholder management skills, a proactive attitude, and self-driven approach that make you a great contributor to any team. Finally, you communicate clearly and effectively making sure you create an impact – either directly or indirectly through others. \\n\\n\\xa0\\n\\nPlease apply at your earliest convenience. We do not have an application deadline but accept applications as long as the job is posted. The job ad will be closed once we have found the right candidate.\\nEmployee Benefits\\rWe are excited to offer you the following benefits with your employment:Bonus systemPaid vacationFlexible working hoursPossibility to work remotelyPension planPersonal insuranceCommunication packageOpportunity to join Employee Resource GroupsState of the art virtual work environmentEmployee Referral ProgramThis list does not promise or guarantee any particular benefit or specific action. They may depend on country or contract specifics and are subject to change at any time without prior notice.Danfoss – Engineering Tomorrow\\rAt Danfoss, we are engineering solutions that allow the world to use resources in smarter ways - driving the sustainable transformation of tomorrow. No transformation has ever been started without a group of passionate, dedicated and empowered people. We believe that innovation and great results are driven by the right mix of people with diverse backgrounds, personalities, skills, and perspectives, reflecting the world in which we do business. To make sure the mix of people works, we strive to create an inclusive work environment where people of all backgrounds are treated equally, respected, and valued for who they are. It is a strong priority within Danfoss to improve the health, working environment and safety of our employees.Following our founder’s mindset “action speaks louder than words”, we set ourselves ambitious targets to protect the environment by embarking on a plan to become CO2 neutral latest by 2030.Danfoss is an EO employer and VEVRAA Federal Contractor. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, age, disability, veteran status, or other protected category.\\n\\nDanfoss engineers solutions that increase machine productivity, reduce emissions, lower energy consumption, and enable electrification.\\n\\nOur solutions are used in such areas as refrigeration, air conditioning, heating, power conversion, motor control, industrial machinery, automotive, marine, and off- and on-highway equipment. We also provide solutions for renewable energy, such as solar and wind power, as well as district-energy infrastructure for cities.\\n\\nOur innovative engineering dates back to 1933. Danfoss is family-owned, employing more than 42.000 people, serving customers in more than 100 countries through a global footprint of 95 factories.\\n\\nOur innovative engineering dates back to 1933. Danfoss is family-owned, employing more than 42.000 people, serving customers in more than 100 countries through a global footprint of 95 factories.\\n\\nDanfoss engineers solutions that increase machine productivity, reduce emissions, lower energy consumption, and enable electrification.\\n\\nOur solutions are used in such areas as refrigeration, air conditioning, heating, power conversion, motor control, industrial machinery, automotive, marine, and off- and on-highway equipment. We also provide solutions for renewable energy, such as solar and wind power, as well as district-energy infrastructure for cities.\\n\\nOur innovative engineering dates back to 1933. Danfoss is family-owned, employing more than 42.000 people, serving customers in more than 100 countries through a global footprint of 95 factories.\\n\\nOur innovative engineering dates back to 1933. Danfoss is family-owned, employing more than 42.000 people, serving customers in more than 100 countries through a global footprint of 95 factories.\\n\\nApply now »\\n\\nFind similar jobs:\\n\\nPrivacy policy\\n\\nDanfoss Job Portal Terms\\n\\nCookies\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCookie Consent Manager\\n\\nWhen you visit any website, it may store or retrieve information on your browser, mostly in the form of cookies. Because we respect your right to privacy, you can choose not to allow some types of cookies. However, blocking some types of cookies may impact your experience of the site and the services we are able to offer.\\n\\nRequired Cookies\\n\\nThese cookies are required to use this website and can\\'t be turned off.\\n\\nProvider \\n                                                     Description \\n                                                     Enabled \\n                                                 \\n                                                         SAP as service provider \\n                                                         \\n                                                             We use the following session cookies, which are all required to enable the website to function: \"route\" is used for session stickiness \"careerSiteCompanyId\" is used to send the request to the correct data centre \"JSESSIONID\" is placed on the visitor\\'s device during the session so the server can identify the visitor \"Load balancer cookie\" (actual cookie name may vary) prevents a visitor from bouncing from one instance to another\\n\\nFunctional Cookies\\n\\nThese cookies provide a better customer experience on this site, such as by remembering your login details, optimising video performance or providing us with information about how our site is used. You may freely choose to accept or decline these cookies at any time. Note that certain functionalities that these third-parties make available may be impacted if you do not accept these cookies.\\n\\nProvider \\n                                                         Description \\n                                                         Enabled \\n                                                     \\n                                                             YouTube \\n                                                             \\n                                                                 YouTube is a video-sharing service where users can create their own profile, upload videos, watch, like and comment on videos. Opting out of YouTube cookies will disable your ability to watch or interact with YouTube videos. Cookie Policy Privacy Policy Terms and Conditions\\n\\nAdvertising Cookies\\n\\nThese cookies serve ads that are relevant to your interests. You may freely choose to accept or decline these cookies at any time. Note that certain functionality that these third parties make available may be impacted if you do not accept these cookies.\\n\\nProvider \\n                                                         Description \\n                                                         Enabled \\n                                                     \\n                                                             Google Analytics \\n                                                             \\n                                                                 Google Analytics is a web analytics service offered by Google that tracks and reports website traffic. Cookie Information Privacy Policy Terms and Conditions \\n                                                                 \\n                                                             \\n                                                             \\n                                                                 \\n                                                                     \\n                                                                     \\n                                                                     \\n                                                                         \\n                                                                             \\n                                                                             \\n                                                                         \\n                                                                     \\n                                                                 \\n                                                             \\n                                                         \\n                                                             Google Tag Manager \\n                                                             \\n                                                                 Google Tag Manager is a tag management system for conversion tracking, site analytics, remarketing and more. Privacy Policy Terms and Conditions \\n                                                                 \\n                                                             \\n                                                             \\n                                                                 \\n                                                                     \\n                                                                     \\n                                                                     \\n                                                                         \\n                                                                             \\n                                                                             \\n                                                                         \\n                                                                     \\n                                                                 \\n                                                             \\n                                                         \\n                                                             LinkedIn \\n                                                             \\n                                                                 LinkedIn is an employment-oriented social networking service. We use the Apply with LinkedIn feature to allow you to apply for jobs using your LinkedIn profile. Opting out of LinkedIn cookies will disable your ability to use Apply with LinkedIn. Cookie Policy Cookie Table Privacy Policy Terms and Conditions \\n                                                                 \\n                                                             \\n                                                             \\n                                                                 \\n                                                                     \\n                                                                     \\n                                                                     \\n                                                                         \\n                                                                             \\n                                                                             \\n                                                                         \\n                                                                     \\n                                                                 \\n                                                             \\n                                                         \\n                                                             AddThis \\n                                                             \\n                                                                 Google Analytics is a web analytics service offered by Google that tracks and reports website traffic. Cookie Information Privacy Policy Terms and Conditions', metadata={'source': 'https://jobs.danfoss.com/job/Copenhagen-AI-Engineer/793304102/'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "458fa34583638604"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:13:11.742833Z",
     "start_time": "2024-05-06T19:13:11.739656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\"\"Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan. \\n\n",
    "    It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine. \\n\n",
    "    Set in a dystopian future where humanity is embroiled in a catastrophic blight and famine, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for humankind.\\n\n",
    "    \\n\n",
    "    Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007 and was originally set to be directed by Steven Spielberg. \\n\n",
    "    Kip Thorne, a Caltech theoretical physicist and 2017 Nobel laureate in Physics,[4] was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar. \\n\n",
    "    Cinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision anamorphic format and IMAX 70 mm. Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles. \\n\n",
    "    Interstellar uses extensive practical and miniature effects, and the company Double Negative created additional digital effects.\\n\n",
    "    \\n\n",
    "    Interstellar premiered in Los Angeles on October 26, 2014. In the United States, it was first released on film stock, expanding to venues using digital projectors. The film received generally positive reviews from critics and grossed over $677 million worldwide ($715 million after subsequent re-releases), making it the tenth-highest-grossing film of 2014. \\n\n",
    "    It has been praised by astronomers for its scientific accuracy and portrayal of theoretical astrophysics.[5][6][7] Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades.\"\"\"\n"
   ],
   "id": "6afe9fb536cf618c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:13:35.776120Z",
     "start_time": "2024-05-06T19:13:35.766884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "new_line_separator = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "chunks = new_line_separator.split_text(text)\n",
    "len(chunks)"
   ],
   "id": "6a0459997808db17",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 214, which is longer than the specified 200\n",
      "Created a chunk of size 202, which is longer than the specified 200\n",
      "Created a chunk of size 212, which is longer than the specified 200\n",
      "Created a chunk of size 362, which is longer than the specified 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:16:16.308308Z",
     "start_time": "2024-05-06T19:16:16.305298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "r_chunks = splitter.split_text(text)\n",
    "len(r_chunks)"
   ],
   "id": "21f91180fd5ddcd3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T19:16:29.379739Z",
     "start_time": "2024-05-06T19:16:29.377855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for chunk in r_chunks:\n",
    "    print(len(chunk))"
   ],
   "id": "252c65d4518032c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "120\n",
      "195\n",
      "14\n",
      "181\n",
      "183\n",
      "13\n",
      "194\n",
      "12\n",
      "128\n",
      "191\n",
      "165\n",
      "193\n",
      "59\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T15:12:32.842267Z",
     "start_time": "2024-05-07T14:35:25.238555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"ibm-granite/granite-3b-code-instruct\")"
   ],
   "id": "5ebc5d902998acd8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/680 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d519cd346b247af9f882394213b6d2c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/41.6k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6881c6742214b90beaecc953411b7ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6496874ef8a48be98053bc1ccc3fd13"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44c0fb5a1b17413e936ad0ec36ef16eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/1.99G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2a136a78f654df28f90fca413e43793"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fbc1a4dc7fa4e03bc4368815e1c75b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ibm-granite/granite-3b-code-instruct were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f429e3e880b444678cf8369b1ef35292"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.54k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0cb1d5dd0e774cddbc8b4ab380fafe14"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac27869b46504f6793389012bbd20958"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a42bc1069a4c46b399e6d4beb54996eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T15:30:29.459484Z",
     "start_time": "2024-05-07T15:30:23.910761Z"
    }
   },
   "cell_type": "code",
   "source": "pipe(\"Tell me a joke\")",
   "id": "1af5903f4220926d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/transformers/generation/utils.py:1141: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Tell me a joke that will make you feel like you have a friend who has been through a'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T08:33:14.006142Z",
     "start_time": "2024-05-08T08:33:14.003766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loader = UnstructuredURLLoader(\n",
    "    urls=[\n",
    "        \"https://www.moneycontrol.com/news/business/banks/hdfc-bank-re-appoints-sanmoy-chakrabarti-as-chief-risk-officer-11259771.html\",\n",
    "        \"https://www.moneycontrol.com/news/business/markets/market-corrects-post-rbi-ups-inflation-forecast-icrr-bet-on-these-top-10-rate-sensitive-stocks-ideas-11142611.html\"\n",
    "    ]\n",
    ")"
   ],
   "id": "af27fe29cdcafb6b",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T08:33:26.937047Z",
     "start_time": "2024-05-08T08:33:20.620962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = loader.load()\n",
    "len(data)"
   ],
   "id": "4766228589838b74",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T08:34:10.163284Z",
     "start_time": "2024-05-08T08:34:10.158252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(data)\n",
    "len(docs)"
   ],
   "id": "8fc6c588615518e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T08:35:29.982972Z",
     "start_time": "2024-05-08T08:35:22.914688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "vector_index_huggingface = FAISS.from_documents(docs, embeddings)"
   ],
   "id": "68545da34db421a7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T08:36:36.986978Z",
     "start_time": "2024-05-08T08:36:36.682835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"vector_index.pkl\"\n",
    "with open(file_path, \"wb\") as f:\n",
    "    pickle.dump(vector_index_huggingface, f)"
   ],
   "id": "90cc0d1ac4b9a6e8",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T08:37:08.211248Z",
     "start_time": "2024-05-08T08:37:07.901425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        vector_index = pickle.load(f)"
   ],
   "id": "f69e30eb97f77aae",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T08:41:10.669847Z",
     "start_time": "2024-05-08T08:40:09.935165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "llm = pipeline(\"text-generation\", model=\"meta-llama/Meta-Llama-Guard-2-8B\",\n",
    "               token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"), device=device)"
   ],
   "id": "73edd2756224006e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53bcbec88ed147b4b4f1221bff7f15f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T08:46:21.472970Z",
     "start_time": "2024-05-08T08:45:58.801821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-Guard-2-8B\"\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "retriever = vector_index.as_retriever()"
   ],
   "id": "12adeb3f2ebf20f0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91fb4d6a7d7745878be63d03eda6fc2f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T08:54:07.145143Z",
     "start_time": "2024-05-08T08:54:06.083321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain import hub\n",
    "\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "combine_docs_chain = create_stuff_documents_chain(llm_model, retrieval_qa_chat_prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
    "retrieval_chain"
   ],
   "id": "854586bcf7af8742",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x6c8ab32f0>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, metadata={'lc_hub_owner': 'langchain-ai', 'lc_hub_repo': 'retrieval-qa-chat', 'lc_hub_commit_hash': 'b60afb6297176b022244feb83066e10ecadcda7b90423654c4a9d45e7a73cebc'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template='Answer any use questions based solely on the context below:\\n\\n<context>\\n{context}\\n</context>')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "            | RunnableLambda(...)\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T08:56:22.020968Z",
     "start_time": "2024-05-08T08:56:21.842161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import langchain\n",
    "\n",
    "query = \"What is the price of Tiago iCNG?\"\n",
    "\n",
    "langchain.debug = True\n",
    "\n",
    "retrieval_chain.invoke({\"question\": query})"
   ],
   "id": "3d0940cfa5861a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:retrieval_chain] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"question\": \"What is the price of Tiago iCNG?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:retrieval_chain > chain:RunnableAssign<context>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"question\": \"What is the price of Tiago iCNG?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context>] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"question\": \"What is the price of Tiago iCNG?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:retrieve_documents] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"question\": \"What is the price of Tiago iCNG?\"\n",
      "}\n",
      "\u001B[32;1m\u001B[1;3m[chain/start]\u001B[0m \u001B[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:retrieve_documents > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001B[0m{\n",
      "  \"question\": \"What is the price of Tiago iCNG?\"\n",
      "}\n",
      "\u001B[31;1m\u001B[1;3m[chain/error]\u001B[0m \u001B[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:retrieve_documents > chain:RunnableLambda] [1ms] Chain run errored with error:\n",
      "\u001B[0m\"KeyError('input')Traceback (most recent call last):\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 1626, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/config.py\\\", line 347, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 3837, in _invoke\\n    output = call_func_with_variable_args(\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/config.py\\\", line 347, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain/chains/retrieval.py\\\", line 61, in <lambda>\\n    retrieval_docs = (lambda x: x[\\\"input\\\"]) | retriever\\n                                ~^^^^^^^^^\\n\\n\\nKeyError: 'input'\"\n",
      "\u001B[31;1m\u001B[1;3m[chain/error]\u001B[0m \u001B[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context> > chain:retrieve_documents] [2ms] Chain run errored with error:\n",
      "\u001B[0m\"KeyError('input')Traceback (most recent call last):\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 2499, in invoke\\n    input = step.invoke(\\n            ^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 3963, in invoke\\n    return self._call_with_config(\\n           ^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 1626, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/config.py\\\", line 347, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 3837, in _invoke\\n    output = call_func_with_variable_args(\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/config.py\\\", line 347, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain/chains/retrieval.py\\\", line 61, in <lambda>\\n    retrieval_docs = (lambda x: x[\\\"input\\\"]) | retriever\\n                                ~^^^^^^^^^\\n\\n\\nKeyError: 'input'\"\n",
      "\u001B[31;1m\u001B[1;3m[chain/error]\u001B[0m \u001B[1m[chain:retrieval_chain > chain:RunnableAssign<context> > chain:RunnableParallel<context>] [6ms] Chain run errored with error:\n",
      "\u001B[0m\"KeyError('input')Traceback (most recent call last):\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 3142, in invoke\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n                   ^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/lib/python3.12/concurrent/futures/_base.py\\\", line 456, in result\\n    return self.__get_result()\\n           ^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/lib/python3.12/concurrent/futures/_base.py\\\", line 401, in __get_result\\n    raise self._exception\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/lib/python3.12/concurrent/futures/thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 4525, in invoke\\n    return self.bound.invoke(\\n           ^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 2499, in invoke\\n    input = step.invoke(\\n            ^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 3963, in invoke\\n    return self._call_with_config(\\n           ^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 1626, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/config.py\\\", line 347, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 3837, in _invoke\\n    output = call_func_with_variable_args(\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/config.py\\\", line 347, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain/chains/retrieval.py\\\", line 61, in <lambda>\\n    retrieval_docs = (lambda x: x[\\\"input\\\"]) | retriever\\n                                ~^^^^^^^^^\\n\\n\\nKeyError: 'input'\"\n",
      "\u001B[31;1m\u001B[1;3m[chain/error]\u001B[0m \u001B[1m[chain:retrieval_chain > chain:RunnableAssign<context>] [8ms] Chain run errored with error:\n",
      "\u001B[0m\"KeyError('input')Traceback (most recent call last):\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 1626, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/config.py\\\", line 347, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/passthrough.py\\\", line 456, in _invoke\\n    **self.mapper.invoke(\\n      ^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 3142, in invoke\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n                   ^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/lib/python3.12/concurrent/futures/_base.py\\\", line 456, in result\\n    return self.__get_result()\\n           ^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/lib/python3.12/concurrent/futures/_base.py\\\", line 401, in __get_result\\n    raise self._exception\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/lib/python3.12/concurrent/futures/thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 4525, in invoke\\n    return self.bound.invoke(\\n           ^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 2499, in invoke\\n    input = step.invoke(\\n            ^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 3963, in invoke\\n    return self._call_with_config(\\n           ^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 1626, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/config.py\\\", line 347, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 3837, in _invoke\\n    output = call_func_with_variable_args(\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/config.py\\\", line 347, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain/chains/retrieval.py\\\", line 61, in <lambda>\\n    retrieval_docs = (lambda x: x[\\\"input\\\"]) | retriever\\n                                ~^^^^^^^^^\\n\\n\\nKeyError: 'input'\"\n",
      "\u001B[31;1m\u001B[1;3m[chain/error]\u001B[0m \u001B[1m[chain:retrieval_chain] [12ms] Chain run errored with error:\n",
      "\u001B[0m\"KeyError('input')Traceback (most recent call last):\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 2499, in invoke\\n    input = step.invoke(\\n            ^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/passthrough.py\\\", line 469, in invoke\\n    return self._call_with_config(self._invoke, input, config, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 1626, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/config.py\\\", line 347, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/passthrough.py\\\", line 456, in _invoke\\n    **self.mapper.invoke(\\n      ^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 3142, in invoke\\n    output = {key: future.result() for key, future in zip(steps, futures)}\\n                   ^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/lib/python3.12/concurrent/futures/_base.py\\\", line 456, in result\\n    return self.__get_result()\\n           ^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/lib/python3.12/concurrent/futures/_base.py\\\", line 401, in __get_result\\n    raise self._exception\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/lib/python3.12/concurrent/futures/thread.py\\\", line 58, in run\\n    result = self.fn(*self.args, **self.kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 4525, in invoke\\n    return self.bound.invoke(\\n           ^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 2499, in invoke\\n    input = step.invoke(\\n            ^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 3963, in invoke\\n    return self._call_with_config(\\n           ^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 1626, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/config.py\\\", line 347, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py\\\", line 3837, in _invoke\\n    output = call_func_with_variable_args(\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/config.py\\\", line 347, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/felixachter/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain/chains/retrieval.py\\\", line 61, in <lambda>\\n    retrieval_docs = (lambda x: x[\\\"input\\\"]) | retriever\\n                                ~^^^^^^^^^\\n\\n\\nKeyError: 'input'\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'input'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[38], line 7\u001B[0m\n\u001B[1;32m      3\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat is the price of Tiago iCNG?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      5\u001B[0m langchain\u001B[38;5;241m.\u001B[39mdebug \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[43mretrieval_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mquestion\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py:4525\u001B[0m, in \u001B[0;36mRunnableBindingBase.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   4519\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m   4520\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4521\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[1;32m   4522\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   4523\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[1;32m   4524\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n\u001B[0;32m-> 4525\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbound\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4526\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4527\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_merge_configs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4528\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4529\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py:2499\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config)\u001B[0m\n\u001B[1;32m   2497\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   2498\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps):\n\u001B[0;32m-> 2499\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2500\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2501\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# mark each step as a child run\u001B[39;49;00m\n\u001B[1;32m   2502\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpatch_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2503\u001B[0m \u001B[43m                \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseq:step:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2504\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2505\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2506\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   2507\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/passthrough.py:469\u001B[0m, in \u001B[0;36mRunnableAssign.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    463\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    464\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    465\u001B[0m     \u001B[38;5;28minput\u001B[39m: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[1;32m    466\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    467\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    468\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[0;32m--> 469\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_with_config\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_invoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py:1626\u001B[0m, in \u001B[0;36mRunnable._call_with_config\u001B[0;34m(self, func, input, config, run_type, **kwargs)\u001B[0m\n\u001B[1;32m   1622\u001B[0m     context \u001B[38;5;241m=\u001B[39m copy_context()\n\u001B[1;32m   1623\u001B[0m     context\u001B[38;5;241m.\u001B[39mrun(var_child_runnable_config\u001B[38;5;241m.\u001B[39mset, child_config)\n\u001B[1;32m   1624\u001B[0m     output \u001B[38;5;241m=\u001B[39m cast(\n\u001B[1;32m   1625\u001B[0m         Output,\n\u001B[0;32m-> 1626\u001B[0m         \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1627\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcall_func_with_variable_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1628\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1629\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1630\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1631\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1632\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1633\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m   1634\u001B[0m     )\n\u001B[1;32m   1635\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1636\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/config.py:347\u001B[0m, in \u001B[0;36mcall_func_with_variable_args\u001B[0;34m(func, input, config, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[1;32m    346\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m run_manager\n\u001B[0;32m--> 347\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/passthrough.py:456\u001B[0m, in \u001B[0;36mRunnableAssign._invoke\u001B[0;34m(self, input, run_manager, config, **kwargs)\u001B[0m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_invoke\u001B[39m(\n\u001B[1;32m    444\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    445\u001B[0m     \u001B[38;5;28minput\u001B[39m: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    448\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    449\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Any]:\n\u001B[1;32m    450\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[1;32m    451\u001B[0m         \u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mdict\u001B[39m\n\u001B[1;32m    452\u001B[0m     ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe input to RunnablePassthrough.assign() must be a dict.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m    455\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m--> 456\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    458\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpatch_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    459\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    461\u001B[0m     }\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py:3142\u001B[0m, in \u001B[0;36mRunnableParallel.invoke\u001B[0;34m(self, input, config)\u001B[0m\n\u001B[1;32m   3129\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m get_executor_for_config(config) \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[1;32m   3130\u001B[0m         futures \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   3131\u001B[0m             executor\u001B[38;5;241m.\u001B[39msubmit(\n\u001B[1;32m   3132\u001B[0m                 step\u001B[38;5;241m.\u001B[39minvoke,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3140\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m key, step \u001B[38;5;129;01min\u001B[39;00m steps\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m   3141\u001B[0m         ]\n\u001B[0;32m-> 3142\u001B[0m         output \u001B[38;5;241m=\u001B[39m {key: \u001B[43mfuture\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresult\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m key, future \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(steps, futures)}\n\u001B[1;32m   3143\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   3144\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/concurrent/futures/_base.py:456\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m--> 456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__get_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    458\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m()\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/concurrent/futures/_base.py:401\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[1;32m    404\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/lib/python3.12/concurrent/futures/thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py:4525\u001B[0m, in \u001B[0;36mRunnableBindingBase.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   4519\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m   4520\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4521\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[1;32m   4522\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   4523\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[1;32m   4524\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n\u001B[0;32m-> 4525\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbound\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4526\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4527\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_merge_configs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4528\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4529\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py:2499\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config)\u001B[0m\n\u001B[1;32m   2497\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   2498\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msteps):\n\u001B[0;32m-> 2499\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2500\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2501\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# mark each step as a child run\u001B[39;49;00m\n\u001B[1;32m   2502\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpatch_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2503\u001B[0m \u001B[43m                \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseq:step:\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2504\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2505\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2506\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   2507\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py:3963\u001B[0m, in \u001B[0;36mRunnableLambda.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   3961\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Invoke this runnable synchronously.\"\"\"\u001B[39;00m\n\u001B[1;32m   3962\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 3963\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_with_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3964\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_invoke\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3965\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3966\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3967\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3968\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3969\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3970\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m   3971\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke a coroutine function synchronously.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3972\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUse `ainvoke` instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3973\u001B[0m     )\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py:1626\u001B[0m, in \u001B[0;36mRunnable._call_with_config\u001B[0;34m(self, func, input, config, run_type, **kwargs)\u001B[0m\n\u001B[1;32m   1622\u001B[0m     context \u001B[38;5;241m=\u001B[39m copy_context()\n\u001B[1;32m   1623\u001B[0m     context\u001B[38;5;241m.\u001B[39mrun(var_child_runnable_config\u001B[38;5;241m.\u001B[39mset, child_config)\n\u001B[1;32m   1624\u001B[0m     output \u001B[38;5;241m=\u001B[39m cast(\n\u001B[1;32m   1625\u001B[0m         Output,\n\u001B[0;32m-> 1626\u001B[0m         \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1627\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcall_func_with_variable_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1628\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1629\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1630\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1631\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1632\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1633\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m   1634\u001B[0m     )\n\u001B[1;32m   1635\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1636\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/config.py:347\u001B[0m, in \u001B[0;36mcall_func_with_variable_args\u001B[0;34m(func, input, config, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[1;32m    346\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m run_manager\n\u001B[0;32m--> 347\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/base.py:3837\u001B[0m, in \u001B[0;36mRunnableLambda._invoke\u001B[0;34m(self, input, run_manager, config, **kwargs)\u001B[0m\n\u001B[1;32m   3835\u001B[0m                 output \u001B[38;5;241m=\u001B[39m chunk\n\u001B[1;32m   3836\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 3837\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mcall_func_with_variable_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3838\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m   3839\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3840\u001B[0m \u001B[38;5;66;03m# If the output is a runnable, invoke it\u001B[39;00m\n\u001B[1;32m   3841\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, Runnable):\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain_core/runnables/config.py:347\u001B[0m, in \u001B[0;36mcall_func_with_variable_args\u001B[0;34m(func, input, config, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[1;32m    346\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m run_manager\n\u001B[0;32m--> 347\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/langchain/chains/retrieval.py:61\u001B[0m, in \u001B[0;36mcreate_retrieval_chain.<locals>.<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m     59\u001B[0m     retrieval_docs: Runnable[\u001B[38;5;28mdict\u001B[39m, RetrieverOutput] \u001B[38;5;241m=\u001B[39m retriever\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 61\u001B[0m     retrieval_docs \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28;01mlambda\u001B[39;00m x: \u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m) \u001B[38;5;241m|\u001B[39m retriever\n\u001B[1;32m     63\u001B[0m retrieval_chain \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     64\u001B[0m     RunnablePassthrough\u001B[38;5;241m.\u001B[39massign(\n\u001B[1;32m     65\u001B[0m         context\u001B[38;5;241m=\u001B[39mretrieval_docs\u001B[38;5;241m.\u001B[39mwith_config(run_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mretrieve_documents\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m     66\u001B[0m     )\u001B[38;5;241m.\u001B[39massign(answer\u001B[38;5;241m=\u001B[39mcombine_docs_chain)\n\u001B[1;32m     67\u001B[0m )\u001B[38;5;241m.\u001B[39mwith_config(run_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mretrieval_chain\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m retrieval_chain\n",
      "\u001B[0;31mKeyError\u001B[0m: 'input'"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T09:05:56.343811Z",
     "start_time": "2024-05-08T09:05:56.102617Z"
    }
   },
   "cell_type": "code",
   "source": "llm_model(\"Tell me a joke\")",
   "id": "6c9b970986e0e86e",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mllm_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTell me a joke\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1211\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[0m\n\u001B[1;32m   1208\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m   1210\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m-> 1211\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1212\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1213\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1214\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1215\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1216\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1217\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1218\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1219\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1220\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1221\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1222\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1224\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1225\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpretraining_tp \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:974\u001B[0m, in \u001B[0;36mLlamaModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[0m\n\u001B[1;32m    971\u001B[0m     use_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    973\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 974\u001B[0m     inputs_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    976\u001B[0m past_seen_tokens \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    977\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:  \u001B[38;5;66;03m# kept for BC (cache positions)\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/torch/nn/modules/sparse.py:163\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 163\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.12.2/envs/article-gpt/lib/python3.12/site-packages/torch/nn/functional.py:2264\u001B[0m, in \u001B[0;36membedding\u001B[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[1;32m   2258\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[1;32m   2259\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[1;32m   2260\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[1;32m   2261\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[1;32m   2262\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[1;32m   2263\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[0;32m-> 2264\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mTypeError\u001B[0m: embedding(): argument 'indices' (position 2) must be Tensor, not str"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aaf4580ccc4a1f90"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
